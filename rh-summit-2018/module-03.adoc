== Change Data Capture with Kafka Connect and Debezium

This part of the lab introduces you to change data capture (CDC) using http://debezium.io/[Debezium].
Debezium allows you to capture data changes from MySQL, Postgres and MongoDB and stream those events into Apache Kafka.
It is based on Kafka Connect.

In the following you'll learn the following things:

* Setting up Kafka Connect with the Debezium CDC connectors
* Setting up the Debezium MySQL connector, ingesting changes from an existing example CRUD application
* Streaming Data Changes to a Postgres Sink Database using the Confluent JDBC sink connector
* Setting up a WildFly Swarm application for consuming the same CDC events and stream them to a web browser via WebSockets

=== Setting Up the Example Application

We've prepared a small CRUD application which we'll use in the following as source of data change events.
It is a Java EE application running on WildFly and using MySQL as its database.

Start a MySQL instance by running:

[source]
oc new-app --name=mysql debezium/example-mysql:0.7
oc env dc/mysql MYSQL_ROOT_PASSWORD=debezium MYSQL_USER=mysqluser MYSQL_PASSWORD=mysqlpw

Then build and start the application.
This uses the https://github.com/openshift-s2i/s2i-wildfly[WildFly S2I] ("source to image") builder image which will fetch the application's source code from the given location,
build it using Maven and deploy it to WildFly, setting up a MySQL datasource using the given credentials:

[source]
oc new-app --name=hikingapp openshift/wildfly-120-centos7:latest~https://github.com/strimzi/strimzi-lab.git \
    --context-dir=hiking-demo \
    -e MYSQL_DATABASE=inventory \
    -e MYSQL_PASSWORD=mysqlpw \
    -e MYSQL_USER=mysqluser \
    -e MYSQL_DATASOURCE=HikingDS
oc expose svc hikingapp

Verify that the pods of the database and the application are running:

[source]
oc get pods

Access the application in your web browser:

[source]
http://hikingapp-default.<YOUR_IP>.xip.io/hikr-1.0-SNAPSHOT/hikes.html

You can use "Populate Data" to create some example data.
Otherwise just insert a few records as you like.

=== Setting Up Kafka Connect With the Debezium Connectors

Deploy Kafka via Strimzi (TODO: should already be done as part of module 1):

[source]
----
export STRIMZI_VERSION=0.2.0
git clone -b $STRIMZI_VERSION https://github.com/strimzi/strimzi
cd strimzi

oc create -f examples/install/cluster-controller && oc create -f examples/templates/cluster-controller

oc new-app strimzi-ephemeral -p CLUSTER_NAME=broker -p ZOOKEEPER_NODE_COUNT=1 -p KAFKA_NODE_COUNT=1 -p KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 -p KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
----

Deploy Kafka Connect With Debezium:

[source]
----
oc new-app strimzi-connect-s2i -p CLUSTER_NAME=debezium -p KAFKA_CONNECT_BOOTSTRAP_SERVERS=broker-kafka:9092 -p KAFKA_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1 -p KAFKA_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1 -p KAFKA_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1

export DEBEZIUM_VERSION=0.7.5
mkdir -p plugins && cd plugins && \
for PLUGIN in {mongodb,mysql,postgres}; do \
    curl http://central.maven.org/maven2/io/debezium/debezium-connector-$PLUGIN/$DEBEZIUM_VERSION/debezium-connector-$PLUGIN-$DEBEZIUM_VERSION-plugin.tar.gz | tar xz; \
done && \
mkdir confluent-jdbc-sink && cd confluent-jdbc-sink && \
curl -O http://central.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar && \
curl -O http://packages.confluent.io/maven/io/confluent/kafka-connect-jdbc/3.3.0/kafka-connect-jdbc-3.3.0.jar && \
cd .. && \
oc start-build debezium-connect --from-dir=. --follow && \
cd .. && rm -rf plugins
----

Register an instance of the Debezium MySQL connector:

[source]
----
oc exec -i broker-kafka-0 -- curl -s -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect:8083/connectors -d @- <<'EOF'

{
    "name": "inventory-connector",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "mysql",
        "database.port": "3306",
        "database.user": "debezium",
        "database.password": "dbz",
        "database.server.id": "184054",
        "database.server.name": "dbserver1",
        "database.whitelist": "inventory",
        "database.history.kafka.bootstrap.servers": "broker-kafka:9092",
        "database.history.kafka.topic": "schema-changes.inventory"
    }
}
EOF
----

This sets up an instance of Debezium's `io.debezium.connector.mysql.MySqlConnector` class,
using the given credentials.

Kafka Connectâ€™s log file should contain messages regarding execution of initial snapshot:

[source]
----
oc logs $(oc get pods -o name -l app=strimzi-connect-s2i)
----

Examine CDC messages in Kafka (add/alter records in the web app and see how messages arrive in the topic):

[source]
----
oc exec -it broker-kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
   --bootstrap-server localhost:9092 \
   --from-beginning \
   --property print.key=true \
   --topic dbserver1.inventory.Hike
----

Using the Kafka Connect REST API, aou also can query the list of connectors, query the status of a given connector, delete a connector and more:

[source]
----
# List all connectors
oc exec -i broker-kafka-0 -- curl -s -X GET \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect:8083/connectors
----

[source]
----
# Get status of "inventory-connector"
oc exec -i broker-kafka-0 -- curl -s -X GET \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect:8083/connectors/inventory-connector/status
----

[source]
----
# Delete "inventory-connector"
oc exec -i broker-kafka-0 -- curl -s -X DELETE \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect:8083/connectors/inventory-connector
----

=== Streaming Data Changes to a Postgres Sink Database

After seeing the change events in the Kafka console, it's time to go a bit further.
Let's set up another database (Postgres in this case) and stream the data changes there.

[source]
----
oc new-app \
    -e POSTGRESQL_USER=postgresuser \
    -e POSTGRESQL_PASSWORD=postgrespw \
    -e POSTGRESQL_DATABASE=inventory \
    centos/postgresql-95-centos7
----

Once the database has started, register an instance of the Confluent JDBC sink connector:

[source]
----
oc exec -i broker-kafka-0 -- curl -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect:8083/connectors -d @- <<'EOF'
{
    "name": "jdbc-sink",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "tasks.max": "1",
        "topics": "dbserver1.inventory.Hike",
        "connection.url": "jdbc:postgresql://postgresql-95-centos7:5432/inventory?user=postgresuser&password=postgrespw",
        "transforms": "unwrap",
        "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
        "auto.create": "true",
        "insert.mode": "upsert",
        "pk.fields": "id",
        "pk.mode": "record_value"
    }
}
EOF
----

This sets up an an instance of `io.confluent.connect.jdbc.JdbcSinkConnector`,
listening to the `dbserver1.inventory.Hike` and streaming all data changes to the given database connection.
As this sink connector just expects the effective state of changed rows
(i.e. the "after" part from the Debezium data change messages),
only this part is extracted using Debezium's `UnwrapFromEnvelope` SMT (single message transform).

With the sink connector being set up, we can take a look into the Postgres database and see how the table changes are propgated there.
Get a shell on the pod of the Postgres service:

[source]
----
oc rsh $(oc get pods -o name -l app=postgresql-95-centos7)
----

Run a query to get all records from the table corresponding to the monitored topic:

[source]
----
psql -U postgresuser inventory -c 'select * from "dbserver1.inventory.Hike"'
----

As you alter records in the source web application,
you'll see how the table in Postgres gets updated accordingly, if you re-execute the query.
Note that `DELETE` operations currently cannot be propagated, as they are not yet supported by the Confluent JDBC sink connector.

To leave the shell on the Postgres pod, run:

[source]
----
exit
----

As an experiment, you also can explore how the streaming approach ensures a loose coupling of the involved components.
Scale down the pods of the "strimzi-connect-s2i" application to 0:

[source]
----
oc scale --replicas=0 dc/debezium-connect
----

You'll still be able to edit records in the source application,
but as Kafka Connect - and with it Debezium - isn't running,
the changes won't be propagated to the sink database.

Once Kafka Connect is restarted again, the connector will automatically pick up where it left before and after a while,
you'll see all changes that had occurred in the connector's downtime in the sink database:

[source]
----
oc scale --replicas=1 dc/debezium-connect
----

=== Consuming Data Change Events With WildFly Swarm

Finally, let's explore how to consume the Debezium events in a custom application and forward them to a web UI using WebSockets.

Deploy the application:

[source]
----
oc new-app --name=websocketsinkapp fabric8/s2i-java:latest~https://github.com/strimzi/strimzi-lab.git \
    --context-dir=debezium-swarm-demo \
    -e MYSQL_DATABASE=inventory \
    -e AB_PROMETHEUS_OFF=true \
    -e KAFKA_SERVICE_HOST=broker-kafka \
    -e KAFKA_SERVICE_PORT=9092

oc expose svc websocketsinkapp
----

One more more must be exposed for the application.
In the OpenShift web console, go to "Applications" -> "Services" -> "websocketsinkapp" -> "Actions" -> "Edit YAML".
Under "spec/ports", add an entry similar to the existing ones for port 8080:

[source]
----
- name: 8080-tcp
  port: 8080
  protocol: TCP
  targetPort: 8080
----

Hit "Save".

Then edit the associated route to expose this new port:
go to "Applications" -> "Routes" -> "websocketsinkapp" -> "Edit".
Change the target port to 8080 and hit "Save".

Visit the application in a new browser window:

[source]
http://websocketsinkapp-default.<YOUR_IP>.xip.io/

Modify some entries in the CRUD application and observe how the change events are progated to the other browser window via WebSockets in near-realtime.
